
"""
Generate test fixtures for the MISER algorithm.

The test fixtures are generated by running many trial 
integrations and saving the mean error and standard
deviation to a pickle file. These can then be tested
for regressions.
"""

from __future__ import print_function
import pickle
import numpy as np
from numpy.random import randint

from skmonaco import mcmiser

miser_functions = { 
        "x**2" : lambda x: x**2,
        "x**2 (sum)" : lambda x: sum(x**2),
        "prod" : lambda x: np.prod(x)
}

DATA_FILE = "miser_data.pkl"

class TestRun(object):

    def __init__(self,fcode,npoints,ntrials,lbound, ubound, analytical_value):
        """
        Create a TestFixture.

        Arguments
        ---------

        fcode : str
            name of function in `miser_functions` dictionary.

        npoints : int
            number of MC integration points in each trial.

        ntrials : int
            number of trials to get approximate values of the mean
            error and standard deviation in error.

        analytical_value : float
            analytical value of the integral.

        lbound, ubound : list of floats
            lower (upper) bound for the integration

        Example
        -------

        To generate a test fixture, 

        >>> fixture = TestRun("x**2", 1e4, 100, [0.], [1.], 1./3.)
        >>> fixture.check_results_error()
        >>> fixture.mean_sigma # average error per trial
        3.86390778798e-05
        >>> fixture.sigma_sigma  # error in error per trial
        3.45037432946e-07
        >>> with open("fixture.pkl") as f:
                pickle.dump(fixture,f)

        Then, to run a regression test,

        >>> import miser_data_generator
        >>> with open("fixture.pkl") as f:
                fixture = pickle.load(f)
        >>> fixture.check_trial_run()
        """
        self.fcode = fcode
        self.f = miser_functions[self.fcode]
        self.npoints = npoints
        self.ntrials = ntrials
        self.analytical_value = analytical_value
        self.lbound = lbound
        self.ubound = ubound
        self.mean_sigma = None # Calculated by self.check_results_error()
        self.sigma_sigma = None # Calculated by self.check_results_error()
        self.seed = randint(0,10000)

    def __getstate__(self):
        result = self.__dict__.copy()
        del result["f"]
        return result

    def __setstate__(self,pickled_dict):
        self.__dict__ = pickled_dict
        self.f = miser_functions[self.fcode]

    def _get_results_errors(self):
        results, errors = [], []
        for itrial in range(self.ntrials):
            res, err = mcmiser(self.f,self.npoints,self.lbound,self.ubound,
                    seed=itrial+self.seed)
            results.append(res)
            errors.append(err)
        results = np.array(results)
        errors = np.array(errors)
        return results, errors

    def check_results_errors(self, verbose=False):
        results, errors = self._get_results_errors()
        mean_result = results.mean()
        mean_error = errors.mean()
        error_in_mean = abs(mean_result-self.analytical_value)
        if verbose:
            print ("  <sigma> = {}; expect < {} (approximately).".format(
                    error_in_mean,mean_error/np.sqrt(self.ntrials)))
        assert error_in_mean<3*mean_error/np.sqrt(self.ntrials),\
                "Error in mean = {}\n\
                 Mean error    = {}".format(
                     error_in_mean, mean_error/np.sqrt(self.ntrials))
        self.mean_sigma = mean_error

    def check_trial_run(self,*args,**kwargs):
        res, err = mcmiser(self.f, self.npoints, self.lbound, self.ubound, *args, **kwargs)
        error_in_mean = abs(res-self.analytical_value)
        assert error_in_mean < 3*self.mean_sigma,\
                "Error in mean  = {}\n\
                 Expected error = {}".format(error_in_mean,self.mean_sigma)



if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(
            description="Generate results for MISER tests.")

    parser.add_argument("-q","--quiet",action="store_true",
            default=False,
            help="Suppress output.")

    parser.add_argument("--only",metavar="X",nargs="+",type=int,
            help="Only run these fixtures.")

    parser.add_argument("--no-save",action="store_true",
            help="Don't save results.")

    args = parser.parse_args()

    VERBOSE = not args.quiet
    
    fixtures = [
            TestRun("x**2", 1e4, 500, [0.],[1.], 1./3.),
            TestRun("x**2", 1e4, 500, [-10.],[3.],1027./3.),
            TestRun("x**2 (sum)", 1e4, 500, [0.]*2,[1.]*2,2./3.),
            TestRun("x**2 (sum)", 1e4, 500, [0.]*6,[1.]*6,2.),
            TestRun("prod", 1e4, 500, [0.],[1.],0.5),
            TestRun("prod", 1e4, 500, [0.]*6,[1.]*6,0.5**6)
    ]

    # If args.only is specified, try loading from data file.
    if args.only is not None:
        try:
            with open(DATA_FILE) as f:
                saved_fixtures = pickle.load(f)
        except IOError:
            print("Failed to load data file: {}".format(DATA_FILE))

    # Prepare the fixtures.
    for ifixture, fixture in enumerate(fixtures):
        if args.only is not None and ifixture not in args.only:
            fixtures[ifixture] = saved_fixtures[ifixture]
            continue
        if VERBOSE:
            msg = "({}) {}".format(ifixture,fixture.fcode)
            print(msg)
            print("="*len(msg))
        fixture.check_results_errors(VERBOSE)
        fixture.check_trial_run()
        if VERBOSE: print()

    # Save the fixtures.
    if not args.no_save:
        with open(DATA_FILE,"w") as f:
            pickle.dump(fixtures,f)
            if VERBOSE: print("Data pickled in {}.".format(DATA_FILE))
    


